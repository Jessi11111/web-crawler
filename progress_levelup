try use LLMExtractionStrategy()

error1:

2025-04-22 15:30:14,371 - ERROR - LLM extraction error for https://www.justinalexander.com/justin-alexander/wedding-dresses/88153/?plp_url=/justin-alexander/wedding-dresses/: litellm.APIConnectionError: 'name'
The litellm.APIConnectionError: 'name' suggests that your application couldn't connect to the Ollama instance running at http://192.168.100.150:11434.


what's the reason the output json is not the same as my schema Dress class ?
after i fix the litellm issue, the output json will be the same as my define schema?
Yes, that's exactly the issue. Your JSON output doesn't match your Dress schema because you're getting an error response from the LLM service instead of actual extracted data.
The error JSON you're seeing ({"index": 0, "error": true, "tags": ["error"], "content": "error message"}) is an error response format from the extraction process, not your schema data.
Once you fix the litellm connection issue with Ollama, the output should match your Dress schema with:
{
  "name": "Wedding Dress Name",
  "description": "Detailed description of the dress",
  "url": "https://example.com/product-page"
}
The litellm error is preventing the LLM from properly processing your extraction request, so instead of getting data that matches your schema, you're getting error responses. Fix the connection to your local Ollama server, and the extraction should work as expected with the correct schema.
-----------------------------
fixed my check the litellm supported model. deepseek-v2 is in it. but deepseek-r1 is not.










error2: (this version is fine, logic no problem, modify as needed) 


2025-04-22 16:57:10,079 - INFO - PDP Link 1: https://www.maggiesottero.com/maggie-sottero/camille/19617
2025-04-22 16:57:10,079 - INFO - PDP Link 2: https://www.maggiesottero.com/maggie-sottero/jace/19639
2025-04-22 16:57:10,079 - INFO - PDP Link 3: https://www.maggiesottero.com/maggie-sottero/anniston-marie-leigh/19372
2025-04-22 16:57:10,254 - INFO - Processing PDP URL: https://www.justinalexander.com/justin-alexander/wedding-dresses/88381PS/?plp_url=/justin-alexander/wedding-dresses/
2025-04-22 16:57:10,255 - INFO - === Starting PDP Data Extraction for https://www.justinalexander.com/justin-alexander/wedding-dresses/88381PS/?plp_url=/justin-alexander/wedding-dresses/ ===

logic happens here in my code:
# Stage 1: Extract and store PDP links
    for url in URLs:
        logger.info(f"Processing PLP URL: {url}")
        await extract_pdp_links(url)

    # Stage 2: Extract data from stored PDP links
    pending_links = collection.find({"status": "pending"})
    for link in pending_links:
        url = link["url"]
        logger.info(f"Processing PDP URL: {url}")








error3:

2025-04-22 17:07:13,683 - INFO - LLM extracted content for https://www.justinalexander.com/justin-alexander/wedding-dresses/88381PS/?plp_url=/justin-alexander/wedding-dresses/:
[]
2025-04-22 17:07:13,684 - INFO - Parsed LLM output for https://www.justinalexander.com/justin-alexander/wedding-dresses/88381PS/?plp_url=/justin-alexander/wedding-dresses/:
[]

=== Token Usage Summary ===
Type                   Count
------------------------------
Completion                 0
Prompt                     0
Total                      0

=== Usage History ===
Request #    Completion       Prompt        Total


The reason you're not seeing any JSON output is because the LLM extraction returned an empty array ([]) instead of actual dress data. 
This indicates that:
The crawler successfully connected to the page
The Ollama LLM connection was established
But NO tokens were used (0 prompt tokens, 0 completion tokens)
This strongly suggests that either:
The page content wasn't properly extracted (possibly due to JavaScript rendering issues)
The extracted content was empty or too small to process
The Ollama model received the request but returned empty results
The empty result isn't a typical error response - it's just empty data. Your code is handling it correctly by validating if data exists before trying to use it, but the root issue is that no extraction is happening.
Try checking if your Ollama model is running correctly and consider adjusting the crawler parameters to ensure content is properly loaded before extraction.









error4:
"content": "litellm.APIConnectionError: 'name'
"content": "litellm.APIConnectionError: OllamaException - {\"error\":\"an error was encountered while running the model: unexpected EOF\"}"


"Unexpected EOF" typically means:
EOF: End Of File
The model crashed or terminated unexpectedly
There was a connection interruption
The model might have run out of memory or encountered another runtime error
This could be caused by:
An issue with the specific model being used
resource constraints on the Ollama server
A problematic input that caused the model to fail


debug1:
(Artemis) jking@DESKTOP-0VKC8HD:/mnt/c/Users/Jessi King/Documents/Artemis$ curl http://192.168.100.150:11434/api/generate -d '{
  "model": "deepseek-v2:latest",
  "prompt": "Hello world",
  "stream": false
}'
{"model":"deepseek-v2:latest","created_at":"2025-04-24T22:18:39.097917387Z","response":" Hello! How can I help you today? If you have any questions or need assistance, feel free to ask.","done":true,"done_reason":"stop","context":[5726,25,37727,1843,185,185,77398,25,37727,0,1724,481,304,1345,340,3571,30,1273,340,463,688,4313,410,933,10972,11,2213,2094,276,2088,13],"total_duration":8400402037,"load_duration":7589903138,"prompt_eval_count":9,"prompt_eval_duration":342000000,"eval_count":24,"eval_duration":466000000}(Artemis)


curl http://192.168.100.150:11434/api/generate -d '{
  "model": "deepseek-v2",
  "prompt": "Hello world",
  "stream": false
}'

{"model":"deepseek-v2","created_at":"2025-04-24T22:38:23.854719493Z","response":" Hello! I'm an AI assistant designed to provide helpful answers and information. If you have any questions or need assistance, feel free to ask.","done":true,"done_reason":"stop","context":[5726,25,37727,1843,185,185,77398,25,37727,0,304,6,76,274,20838,20308,5405,276,2774,9394,9789,285,1757,13,1273,340,463,688,4313,410,933,10972,11,2213,2094,276,2088,13],"total_duration":13822661991,"load_duration":12820490043,"prompt_eval_count":9,"prompt_eval_duration":357000000,"eval_count":31,"eval_duration":643000000}(Artemis)


debug2:
$ ollama list
Command 'ollama' not found, but can be installed with:
sudo snap install ollama
$ curl http://localhost:11434/api/generate -d '{
  "model": "deepseek-v2",
  "prompt": "who are you?",
  "stream": false,
  "format": "json"
}'
{"model":"deepseek-v2","created_at":"2025-04-29T20:47:59.691282355Z","response":"{\"init_msg\":\"You are communicating with an AI assistant. How can I help you?\"}","done":true,"done_reason":"stop","context":[5726,25,779,418,340,30,185,185,77398,25,19332,3006,62,12411,14462,2054,418,35363,366,274,20838,20308,13,1724,481,304,1345,340,1967,92],"total_duration":540164609,"load_duration":7314835,"prompt_eval_count":11,"prompt_eval_duration":106000000,"eval_count":21,"eval_duration":425000000}(Artemis)



debug3(llama3.3):
$ curl http://localhost:11434/api/generate -d '{
  "model": "llama3.3",
  "prompt": "Return a JSON object with a \"content\" field containing a sentence about dogs.",
  "format": "json",
  "stream": false
}'
{"model":"llama3.3","created_at":"2025-04-29T21:02:20.58846326Z","response":"{\"content\": \"Dogs are often considered to be man's best friend.\"}","done":true,"done_reason":"stop","context":[128006,882,128007,271,5715,264,4823,1665,449,264,330,1834,1,2115,8649,264,11914,922,12875,13,128009,128006,78191,128007,271,5018,1834,794,330,35,27403,527,3629,6646,311,387,893,596,1888,4333,1210,92],"total_duration":65821274728,"load_duration":37338478056,"prompt_eval_count":26,"prompt_eval_duration":13961000000,"eval_count":18,"eval_duration":14514000000}(Artemis)




after debug: ?????????????
Based on your tests, there appears to be a fundamental issue with your Ollama setup. Let's clarify and fix this:

Ollama Not Installed System-Wide:
The ollama list command fails because Ollama isn't installed on your host system
It's only running inside your Docker container

Connection Issues:
The curl command fails with "unexpected EOF" suggesting either:
The API endpoint isn't properly exposed from Docker
There's a networking/firewall issue

Docker Configuration:
Since you can access Ollama via OpenWebUI in Docker, but not directly via curl, the API port likely isn't properly mapped












