1. frist, tried throw plp links to LLM by setting up the prompt to get the pdp links output in json file. I modified lots of prompt version. no one worked.
PLP_PROMPT = """Analyze the plp_content(html) above and extract **ALL** valid Product Detail Page (PDP) URLs.  

        ### Definitions:
        - **PDP (Product Detail Page)**: 
        A URL linking to a *specific product* with:
        1. Base path: `/brand/wedding-dresses/`
        2. Contains either:
            - A product ID (e.g., `/88415/`)
            - A product name slug (e.g., `/ballgown-dress/`)
        3. *May include* `?plp_url=...` or other query parameters.

        ### Rules:
        1. **Include ONLY URLs** matching ALL these criteria:
        - Domain: 'https://www.allurebridals.com', 'https://www.essensedesigns.com', 'https://www.maggiesottero.com','https://justinalexander.com'
        - Structure: Has a product ID or name (e.g., `/88415/` or `/ballgown-dress/`)
        - Examples:
            ✅ `https://justinalexander.com/justin-alexander/wedding-dresses/88415/?plp_url=...`
            ✅ `https://justinalexander.com/justin-alexander/wedding-dresses/ballgown-dress/`
            ❌ `https://justinalexander.com/justin-alexander/wedding-dresses/` (PLP, not PDP)
            ❌ `https://justinalexander.com/cart/` (non-product page)

        2. **Exclude**:
        - Duplicates, filters, or non-product links.
        - URLs without a product ID/name in the path.

        3. **Return JSON** (no other text):
        ```json
        {{"pdp_links": ["url1", "url2"]}} 
        Content: {plp_content}"""


failed because LLM can't process links like a browser. we need to feed LLM at least some data in order to extract data.
..............................................................................................................................................................................................................................


2. use crawl4ai to process plp links to get pdp links ( use reasult.link() ). Important: those result.link needed clean and pattern match. I did it!!! At least for the lazy load plp page. I got 150 pdp links from Justin.
And then use crawl4ai to process each pdp link to get some result( either result.markdown, result.cleaned_html, result_extracted_content )
And then throw those results to LLM to extract info that we need. For example:
    "id": "product_id",
    "Name": "product_name",
    "Description": "product_description",
    "crawl_date": "YYYY-MM-DD HH:MM:SS"


failed (reason unknown)
2025-04-04 15:29:40,601 - ERROR - Error processing PDP https://www.justinalexander.com/justin-alexander/wedding-dresses/88334/?plp_url=/justin-alexander/wedding-dresses/: BrowserType.launch: Target page, context or browser has been closed

potential cause:
          Resource Management Issues   -->???
          Timeout Issues               --> fix some config
          Memory/Resource Exhaustion   --> divide the pdp links into smaller batch, batch_size = 5


>>>>>>>>>>>>>>>>>>>>>>>>>>>> OKAYYYY!!!  LOL 
this error is caused by Crawl4ai!!!
----------------------------------------------------------------------------------
aravindkarnam 3 days ago Collaborator
@LLazzar @JohayerChowdury Yes. This issue was already reported in #842 . It's already root caused and patched. Will be released shortly. Closing this issue as duplicate, so that progress can be tracked in a single issue on #842.
-----------------------------------------------------------------------------------
................................................................................................................................................................................................................................


3. I tried put all pdp links to mongo collection that i new started under the database competitors to store partial data that we had now
 pdp links in mongodb:
        _id:67f034621a4a39537c8097b9
        url:"https://www.justinalexander.com/justin-alexander/wedding-dresses/88365…"
        found_date:"2025-04-04 14:34:58"
        status:"pending"
        first_found:"2025-04-04 14:34:58"



4. use crawl4ai again, had this error caused by crawl4ai:BrowserType.launch: Target page, context or browser has been closed
fixed refers to link:https://github.com/unclecode/crawl4ai/issues/842#issuecomment-2778772791
-------------------
Solution
We modified the close() method in the AsyncPlaywrightCrawlerStrategy class to reset the Playwright instance after cleanup:
from crawl4ai.async_crawler_strategy import AsyncPlaywrightCrawlerStrategy
from crawl4ai.browser_manager import BrowserManager


async def patched_async_playwright__crawler_strategy_close(self) -> None:
    """
    Close the browser and clean up resources.

    This patch addresses an issue with Playwright instance cleanup where the static instance
    wasn't being properly reset, leading to issues with multiple crawls.

    Issue: https://github.com/unclecode/crawl4ai/issues/842

    Returns:
        None
    """
    await self.browser_manager.close()

    # Reset the static Playwright instance
    BrowserManager._playwright_instance = None
AsyncPlaywrightCrawlerStrategy.close = patched_async_playwright__crawler_strategy_close
------------------------


5. still have error with the result.
three attribututes of result:
                                 logger.info("=== Content Types Available ===")
                                logger.info(f"Has markdown: {hasattr(pdp_result, 'markdown')}")
                                logger.info(f"Has cleaned_html: {hasattr(pdp_result, 'cleaned_html')}")
                                logger.info(f"Has extracted_content: {hasattr(pdp_result, 'extracted_content')}")
the extracted_content si what i want. 
but result.extracted_content is base on JsonCssExtractionStrategy

A JSON-based extraction strategy refers to a structured way of extracting data from web pages (or raw HTML) by defining a schema (a set of rules) in JSON format. This schema tells the crawler what to extract (e.g., product titles, prices, descriptions) and how to extract it (using CSS selectors, XPath, or LLM-based parsing).

The extracted data is then stored in result.extracted_content as a JSON string (or sometimes plain text) rather than in markdown (result.markdown).



6. fixed by adding schema and fix the strategy type.
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
extraction_strategy=JsonCssExtractionStrategy(WEDDING_DRESS_SCHEMA)
extracted_data = json.loads(pdp_result.extracted_content)


finally worked!!!



























