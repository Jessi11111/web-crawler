1. frist, tried throw plp links to LLM by setting up the prompt to get the pdp links output in json file. I modified lots of prompt version. no one worked.
PLP_PROMPT = """Analyze the plp_content(html) above and extract **ALL** valid Product Detail Page (PDP) URLs.  

        ### Definitions:
        - **PDP (Product Detail Page)**: 
        A URL linking to a *specific product* with:
        1. Base path: `/brand/wedding-dresses/`
        2. Contains either:
            - A product ID (e.g., `/88415/`)
            - A product name slug (e.g., `/ballgown-dress/`)
        3. *May include* `?plp_url=...` or other query parameters.

        ### Rules:
        1. **Include ONLY URLs** matching ALL these criteria:
        - Domain: 'https://www.allurebridals.com', 'https://www.essensedesigns.com', 'https://www.maggiesottero.com','https://justinalexander.com'
        - Structure: Has a product ID or name (e.g., `/88415/` or `/ballgown-dress/`)
        - Examples:
            ✅ `https://justinalexander.com/justin-alexander/wedding-dresses/88415/?plp_url=...`
            ✅ `https://justinalexander.com/justin-alexander/wedding-dresses/ballgown-dress/`
            ❌ `https://justinalexander.com/justin-alexander/wedding-dresses/` (PLP, not PDP)
            ❌ `https://justinalexander.com/cart/` (non-product page)

        2. **Exclude**:
        - Duplicates, filters, or non-product links.
        - URLs without a product ID/name in the path.

        3. **Return JSON** (no other text):
        ```json
        {{"pdp_links": ["url1", "url2"]}} 
        Content: {plp_content}"""


failed because LLM can't process links like a browser. we need to feed LLM at least some data in order to extract data.
..............................................................................................................................................................................................................................


2. use crawl4ai to process plp links to get pdp links ( use reasult.link() ). Important: those result.link needed clean and pattern match. I did it!!! At least for the lazy load plp page. I got 150 pdp links from Justin.
And then use crawl4ai to process each pdp link to get some result( either result.markdown, result.cleaned_html, result_extracted_content )
And then throw those results to LLM to extract info that we need. For example:
    "id": "product_id",
    "Name": "product_name",
    "Description": "product_description",
    "crawl_date": "YYYY-MM-DD HH:MM:SS"


failed (reason unknown)
2025-04-04 15:29:40,601 - ERROR - Error processing PDP https://www.justinalexander.com/justin-alexander/wedding-dresses/88334/?plp_url=/justin-alexander/wedding-dresses/: BrowserType.launch: Target page, context or browser has been closed

potential cause:
          Resource Management Issues   -->???
          Timeout Issues               --> fix some config
          Memory/Resource Exhaustion   --> divide the pdp links into smaller batch, batch_size = 5


>>>>>>>>>>>>>>>>>>>>>>>>>>>> OKAYYYY!!!  LOL 
this error is caused by Crawl4ai!!!
----------------------------------------------------------------------------------
aravindkarnam 3 days ago Collaborator
@LLazzar @JohayerChowdury Yes. This issue was already reported in #842 . It's already root caused and patched. Will be released shortly. Closing this issue as duplicate, so that progress can be tracked in a single issue on #842.
-----------------------------------------------------------------------------------
................................................................................................................................................................................................................................


3. I tried put all pdp links to mongo collection that i new started under the database competitors to store partial data that we had now
 pdp links in mongodb:
        _id:67f034621a4a39537c8097b9
        url:"https://www.justinalexander.com/justin-alexander/wedding-dresses/88365…"
        found_date:"2025-04-04 14:34:58"
        status:"pending"
        first_found:"2025-04-04 14:34:58"



4. use crawl4ai again, had this error caused by crawl4ai:BrowserType.launch: Target page, context or browser has been closed
fixed refers to link:https://github.com/unclecode/crawl4ai/issues/842#issuecomment-2778772791
-------------------
Solution
We modified the close() method in the AsyncPlaywrightCrawlerStrategy class to reset the Playwright instance after cleanup:
from crawl4ai.async_crawler_strategy import AsyncPlaywrightCrawlerStrategy
from crawl4ai.browser_manager import BrowserManager


async def patched_async_playwright__crawler_strategy_close(self) -> None:
    """
    Close the browser and clean up resources.

    This patch addresses an issue with Playwright instance cleanup where the static instance
    wasn't being properly reset, leading to issues with multiple crawls.

    Issue: https://github.com/unclecode/crawl4ai/issues/842

    Returns:
        None
    """
    await self.browser_manager.close()

    # Reset the static Playwright instance
    BrowserManager._playwright_instance = None
AsyncPlaywrightCrawlerStrategy.close = patched_async_playwright__crawler_strategy_close
------------------------


5. still have error with the result.
three attribututes of result:
                                 logger.info("=== Content Types Available ===")
                                logger.info(f"Has markdown: {hasattr(pdp_result, 'markdown')}")
                                logger.info(f"Has cleaned_html: {hasattr(pdp_result, 'cleaned_html')}")
                                logger.info(f"Has extracted_content: {hasattr(pdp_result, 'extracted_content')}")
the extracted_content si what i want. 
but result.extracted_content is base on JsonCssExtractionStrategy

A JSON-based extraction strategy refers to a structured way of extracting data from web pages (or raw HTML) by defining a schema (a set of rules) in JSON format. This schema tells the crawler what to extract (e.g., product titles, prices, descriptions) and how to extract it (using CSS selectors, XPath, or LLM-based parsing).

The extracted data is then stored in result.extracted_content as a JSON string (or sometimes plain text) rather than in markdown (result.markdown).



6. fixed by adding schema and fix the strategy type.
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
extraction_strategy=JsonCssExtractionStrategy(WEDDING_DRESS_SCHEMA)
extracted_data = json.loads(pdp_result.extracted_content)


finally worked!!! 04/08/25





7.crashed again~~ 04/11/25
---------------------------------------------------------
find out our proxy ran out of bandwidth.

 we exhausted the allocated data transfer limit provided by your proxy service for a given period (usually monthly)
---------------------------------------------------------
andwidth Limit Reached – Proxy services often impose a cap on how much data can pass through their servers. Once you exceed this limit, the proxy will stop working until the next billing cycle or until you purchase additional bandwidth. we reached 5/5 gb
========================================
PROXY_USER = os.environ.get('PROXY_USER', 'web_crawler_gOE9S')
PROXY_PASS = os.environ.get('PROXY_PASS', 'Decrease_Subtotal6_Stencil')
PROXY_SERVER = f"http://customer-{PROXY_USER}-cc-us-sessid-0821780918-sesstime-10:{PROXY_PASS}@pr.oxylabs.io:7777"

browser_config = BrowserConfig(
                headless=True,
                proxy=PROXY_SERVER,
                ...

solution: added more GB. Now added to 20GB.
it worked with some at least!!!


8.
some of the pdp links doesn't work
error
-----------------------------------------------------------------------------------------------------------------------------
Unexpected error in _crawl_web at line 582 in _crawl_web (lib/python3.10/site-                                      │
│ packages/crawl4ai/async_crawler_strategy.py):                                                                         │
│   Error: Failed on navigating ACS-GOTO:                                                                               │
│   Page.goto: net::ERR_TUNNEL_CONNECTION_FAILED at https://www.justinalexander.com/justin-alexander/wedding-           │
│ dresses/88261PS/?plp_url=/justin-alexander/wedding-dresses/                                                           │
│   Call log:                                                                                                           │
│   - navigating to "https://www.justinalexander.com/justin-alexander/wedding-dresses/88261PS/?plp_url=/justin-         │
│ alexander/wedding-dresses/", waiting until "domcontentloaded"                                                         │
│                                                                                                                       │
│                                                                                                                       │
│   Code context:                                                                                                       │
│   577                       response = await page.goto(                                                               │
│   578                           url, wait_until=config.wait_until, timeout=config.page_timeout                        │
│   579                       )                                                                                         │
│   580                       redirected_url = page.url                                                                 │
│   581                   except Error as e:                                                                            │
│   582 →                     raise RuntimeError(f"Failed on navigating ACS-GOTO:\n{str(e)}")                           │
│   583                                                                                                                 │
│   584                   await self.execute_hook(                                                                      │
│   585                       "after_goto", page, context=context, url=url, response=response, config=config            │
│   586                   )                                                                                             │
│   587                                             
--------------------------------------------------------------------------------------------------------------------------------
from github(crawl4ai develpoer):This is not a crawl4AI issue. Sometimes the IP addresses of these proxy providers are blocked by firewalls. Even I faced this exact issue when I connect through my office network, but from my home network or mobile hotspot the proxy connection works fine. Change your network or change the protocol to https://, and see if the issue gets resolved.

issue: proxy provider are blocked by firewalls




































